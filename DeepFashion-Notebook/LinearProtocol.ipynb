{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af7c7133-5c79-45e8-b981-0e4b8ec87ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    cudnn.benchmark = False\n",
    "    cudnn.deterministic = True\n",
    "    random.seed(seed)\n",
    "    \n",
    "SEED = 333\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40457112-62c0-41c7-80d4-b901ec4dbec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataload import get_inshop_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DATASET_DIR = '../../../dataset/InShop/'\n",
    "all_data, label_enc, cat_enc, subcat_enc, vari_enc = get_inshop_dataset(root_path=DATASET_DIR)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid, cat_tr, cat_val = train_test_split(all_data, \n",
    "                                                            label_enc, \n",
    "                                                            cat_enc,\n",
    "                                                            test_size=0.15, \n",
    "                                                            random_state=SEED, \n",
    "                                                            stratify=cat_enc)\n",
    "\n",
    "X_valid, X_test, y_valid, y_test, _, _ = train_test_split(X_valid, \n",
    "                                                            y_valid, \n",
    "                                                            cat_val,\n",
    "                                                            test_size=0.5, \n",
    "                                                            random_state=SEED, \n",
    "                                                            stratify=cat_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7603e22-67e9-48ca-af3f-51791bbc9276",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import MyDataset, TransformsCE\n",
    "\n",
    "size = 224\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = MyDataset(img_list=X_train, \n",
    "                          label_list=y_train, \n",
    "                          phase='train', \n",
    "                          transform=TransformsCE(size))\n",
    "\n",
    "val_dataset = MyDataset(img_list=X_valid, \n",
    "                          label_list=y_valid, \n",
    "                          phase='val', \n",
    "                          transform=TransformsCE(size))\n",
    "\n",
    "test_dataset = MyDataset(img_list=X_test, \n",
    "                          label_list=y_test, \n",
    "                          phase='test', \n",
    "                          transform=TransformsCE(size))\n",
    "\n",
    "image_datasets = {'train' : train_dataset, 'val' : val_dataset, 'test': test_dataset}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
    "\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "                                                                                                                                             \n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "dataloaders = {'train': train_dataloader, 'val': val_dataloader, 'test': test_dataloader}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bae92a-8807-4999-bdfc-34eb892e2d68",
   "metadata": {},
   "source": [
    "-----\n",
    "### HIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff3cccca-5de9-4264-b9db-af8b00792f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda:0 - DeepFashion-pre-linear / ResNet50-HIE-33-400-linear\n"
     ]
    }
   ],
   "source": [
    "SEED = 333\n",
    "seed_everything(SEED)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "MODEL_PATH = '../../../pretrained/ResNet50-HIE-33+300-400.pth'\n",
    "SAVE_PATH = '../../../weights'\n",
    "\n",
    "RUN_NAME = 'ResNet50-HIE-33-400-linear'\n",
    "WANDB_PRJ = 'DeepFashion-pre-linear'\n",
    "\n",
    "CONFIG = {\n",
    "    \"dataset\": \"DeepFashion\",\n",
    "    \"machine\": \"offline cluster\",\n",
    "    \"model\": \"ResNet50-HIE\",\n",
    "    \"learning_rate\": 0.2,\n",
    "    \"batch_size\": batch_size*2\n",
    "}\n",
    "\n",
    "print(f'Training on {device} - {WANDB_PRJ} / {RUN_NAME}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21e02124-4f79-40cb-b1e1-1766cf22d09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 2.0000e-02.\n",
      "Epoch 00000: adjusting learning rate of group 0 to 2.0000e-01.\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from src.models import MLP\n",
    "from src.optim_lars import LARS\n",
    "\n",
    "num_classes = 0\n",
    "model = timm.create_model('resnet50', pretrained=False, num_classes=num_classes)\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "\n",
    "mlp = MLP(num_classes=17) \n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = LARS(\n",
    "    list(mlp.parameters()),\n",
    "    lr=0.2,\n",
    "    weight_decay=1e-6,\n",
    "    exclude_from_weight_decay=['batch_normalization', 'bias'],\n",
    ")\n",
    "\n",
    "warmupscheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch : (epoch+1)/10.0, verbose = True)\n",
    "mainscheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 500, eta_min=0.05, last_epoch=-1, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57de5c42-0dc2-46ef-9557-13cfb6a95e06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m33h002\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/soohee/Workspace/cv/git/MLDL-Prj/ImgR/wandb/run-20221201_130801-1ijdktwn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/33h002/DeepFashion-pre-linear/runs/1ijdktwn\" target=\"_blank\">ResNet50-HIE-33-400-linear</a></strong> to <a href=\"https://wandb.ai/33h002/DeepFashion-pre-linear\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 4.0000e-02.\n",
      "e: 1 | Train Loss: 149824.8254 | 3.3439 | Acc: 0.6641\n",
      "e: 1 | Valid Loss: 9653.5415 | 2.4421 | Acc: 0.7293\n",
      "Validation loss decreased (inf --> 9653.541527).\n",
      "Adjusting learning rate of group 0 to 6.0000e-02.\n",
      "e: 2 | Train Loss: 100262.6889 | 2.2378 | Acc: 0.7816\n",
      "e: 2 | Valid Loss: 8987.4838 | 2.2736 | Acc: 0.7442\n",
      "Validation loss decreased (9653.541527 --> 8987.483818).\n",
      "Adjusting learning rate of group 0 to 8.0000e-02.\n",
      "e: 3 | Train Loss: 91467.4355 | 2.0415 | Acc: 0.7872\n",
      "e: 3 | Valid Loss: 8302.0381 | 2.1002 | Acc: 0.7554\n",
      "Validation loss decreased (8987.483818 --> 8302.038136).\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "e: 4 | Train Loss: 82194.6284 | 1.8345 | Acc: 0.7993\n",
      "e: 4 | Valid Loss: 7661.6352 | 1.9382 | Acc: 0.7647\n",
      "Validation loss decreased (8302.038136 --> 7661.635233).\n",
      "Adjusting learning rate of group 0 to 1.2000e-01.\n",
      "e: 5 | Train Loss: 73196.1758 | 1.6337 | Acc: 0.8141\n",
      "e: 5 | Valid Loss: 7107.0442 | 1.7979 | Acc: 0.7774\n",
      "Validation loss decreased (7661.635233 --> 7107.044174).\n",
      "Adjusting learning rate of group 0 to 1.4000e-01.\n",
      "e: 6 | Train Loss: 66238.1609 | 1.4784 | Acc: 0.8252\n",
      "e: 6 | Valid Loss: 6675.7740 | 1.6888 | Acc: 0.7850\n",
      "Validation loss decreased (7107.044174 --> 6675.774015).\n",
      "Adjusting learning rate of group 0 to 1.6000e-01.\n",
      "e: 7 | Train Loss: 59507.9128 | 1.3282 | Acc: 0.8386\n",
      "e: 7 | Valid Loss: 6348.5684 | 1.6060 | Acc: 0.7938\n",
      "Validation loss decreased (6675.774015 --> 6348.568361).\n",
      "Adjusting learning rate of group 0 to 1.8000e-01.\n",
      "e: 8 | Train Loss: 55178.1915 | 1.2315 | Acc: 0.8463\n",
      "e: 8 | Valid Loss: 6157.5445 | 1.5577 | Acc: 0.7974\n",
      "Validation loss decreased (6348.568361 --> 6157.544456).\n",
      "Adjusting learning rate of group 0 to 2.0000e-01.\n",
      "e: 9 | Train Loss: 51658.2510 | 1.1530 | Acc: 0.8530\n",
      "e: 9 | Valid Loss: 5967.3673 | 1.5096 | Acc: 0.8039\n",
      "Validation loss decreased (6157.544456 --> 5967.367276).\n",
      "Adjusting learning rate of group 0 to 2.2000e-01.\n",
      "e: 10 | Train Loss: 48243.6511 | 1.0767 | Acc: 0.8589\n",
      "e: 10 | Valid Loss: 5867.3218 | 1.4843 | Acc: 0.8052\n",
      "Validation loss decreased (5967.367276 --> 5867.321755).\n",
      "Epoch 00001: adjusting learning rate of group 0 to 2.0000e-01.\n",
      "e: 11 | Train Loss: 46326.6334 | 1.0340 | Acc: 0.8605\n",
      "e: 11 | Valid Loss: 5790.0364 | 1.4647 | Acc: 0.8070\n",
      "Validation loss decreased (5867.321755 --> 5790.036380).\n",
      "Epoch 00002: adjusting learning rate of group 0 to 1.9999e-01.\n",
      "e: 12 | Train Loss: 45453.3904 | 1.0145 | Acc: 0.8633\n",
      "e: 12 | Valid Loss: 5701.0980 | 1.4422 | Acc: 0.8082\n",
      "Validation loss decreased (5790.036380 --> 5701.098003).\n",
      "Epoch 00003: adjusting learning rate of group 0 to 1.9999e-01.\n",
      "e: 13 | Train Loss: 42994.6289 | 0.9596 | Acc: 0.8693\n",
      "e: 13 | Valid Loss: 5645.0839 | 1.4281 | Acc: 0.8131\n",
      "Validation loss decreased (5701.098003 --> 5645.083876).\n",
      "Epoch 00004: adjusting learning rate of group 0 to 1.9998e-01.\n",
      "e: 14 | Train Loss: 42994.5183 | 0.9596 | Acc: 0.8667\n",
      "e: 14 | Valid Loss: 5650.3446 | 1.4294 | Acc: 0.8110\n",
      "Epoch 00005: adjusting learning rate of group 0 to 1.9996e-01.\n",
      "e: 15 | Train Loss: 41093.1117 | 0.9172 | Acc: 0.8723\n",
      "e: 15 | Valid Loss: 5599.9749 | 1.4166 | Acc: 0.8138\n",
      "Validation loss decreased (5645.083876 --> 5599.974942).\n",
      "Epoch 00006: adjusting learning rate of group 0 to 1.9995e-01.\n",
      "e: 16 | Train Loss: 41476.4291 | 0.9257 | Acc: 0.8702\n",
      "e: 16 | Valid Loss: 5590.5919 | 1.4143 | Acc: 0.8143\n",
      "Validation loss decreased (5599.974942 --> 5590.591949).\n",
      "Epoch 00007: adjusting learning rate of group 0 to 1.9993e-01.\n",
      "e: 17 | Train Loss: 40804.6622 | 0.9107 | Acc: 0.8720\n",
      "e: 17 | Valid Loss: 5566.9672 | 1.4083 | Acc: 0.8156\n",
      "Validation loss decreased (5590.591949 --> 5566.967239).\n",
      "Epoch 00008: adjusting learning rate of group 0 to 1.9991e-01.\n",
      "e: 18 | Train Loss: 40165.7526 | 0.8965 | Acc: 0.8722\n",
      "e: 18 | Valid Loss: 5501.6775 | 1.3918 | Acc: 0.8171\n",
      "Validation loss decreased (5566.967239 --> 5501.677483).\n",
      "Epoch 00009: adjusting learning rate of group 0 to 1.9988e-01.\n",
      "e: 19 | Train Loss: 40172.9879 | 0.8966 | Acc: 0.8718\n",
      "e: 19 | Valid Loss: 5494.4500 | 1.3899 | Acc: 0.8163\n",
      "Validation loss decreased (5501.677483 --> 5494.450037).\n",
      "Epoch 00010: adjusting learning rate of group 0 to 1.9985e-01.\n",
      "e: 20 | Train Loss: 39451.4619 | 0.8805 | Acc: 0.8742\n",
      "e: 20 | Valid Loss: 5494.3121 | 1.3899 | Acc: 0.8171\n",
      "Validation loss decreased (5494.450037 --> 5494.312114).\n",
      "Epoch 00011: adjusting learning rate of group 0 to 1.9982e-01.\n",
      "e: 21 | Train Loss: 38586.2836 | 0.8612 | Acc: 0.8747\n",
      "e: 21 | Valid Loss: 5501.1737 | 1.3916 | Acc: 0.8176\n",
      "Epoch 00012: adjusting learning rate of group 0 to 1.9979e-01.\n",
      "e: 22 | Train Loss: 38115.5469 | 0.8507 | Acc: 0.8764\n",
      "e: 22 | Valid Loss: 5516.7018 | 1.3956 | Acc: 0.8189\n",
      "Epoch 00013: adjusting learning rate of group 0 to 1.9975e-01.\n",
      "e: 23 | Train Loss: 37982.9247 | 0.8477 | Acc: 0.8768\n",
      "e: 23 | Valid Loss: 5479.1777 | 1.3861 | Acc: 0.8196\n",
      "Validation loss decreased (5494.312114 --> 5479.177737).\n",
      "Epoch 00014: adjusting learning rate of group 0 to 1.9971e-01.\n",
      "e: 24 | Train Loss: 37381.6072 | 0.8343 | Acc: 0.8782\n",
      "e: 24 | Valid Loss: 5525.4789 | 1.3978 | Acc: 0.8179\n",
      "Epoch 00015: adjusting learning rate of group 0 to 1.9967e-01.\n",
      "e: 25 | Train Loss: 37374.5778 | 0.8342 | Acc: 0.8777\n",
      "e: 25 | Valid Loss: 5472.1550 | 1.3843 | Acc: 0.8179\n",
      "Validation loss decreased (5479.177737 --> 5472.155046).\n",
      "Epoch 00016: adjusting learning rate of group 0 to 1.9962e-01.\n",
      "e: 26 | Train Loss: 37425.4703 | 0.8353 | Acc: 0.8777\n",
      "e: 26 | Valid Loss: 5545.2750 | 1.4028 | Acc: 0.8184\n",
      "Epoch 00017: adjusting learning rate of group 0 to 1.9957e-01.\n",
      "e: 27 | Train Loss: 36706.6355 | 0.8193 | Acc: 0.8797\n",
      "e: 27 | Valid Loss: 5492.5398 | 1.3895 | Acc: 0.8168\n",
      "Epoch 00018: adjusting learning rate of group 0 to 1.9952e-01.\n",
      "e: 28 | Train Loss: 36652.8273 | 0.8181 | Acc: 0.8802\n",
      "e: 28 | Valid Loss: 5506.7793 | 1.3931 | Acc: 0.8204\n",
      "Epoch 00019: adjusting learning rate of group 0 to 1.9947e-01.\n",
      "e: 29 | Train Loss: 36502.8966 | 0.8147 | Acc: 0.8791\n",
      "e: 29 | Valid Loss: 5479.3456 | 1.3861 | Acc: 0.8211\n",
      "Epoch 00020: adjusting learning rate of group 0 to 1.9941e-01.\n",
      "e: 30 | Train Loss: 36288.8542 | 0.8099 | Acc: 0.8800\n",
      "e: 30 | Valid Loss: 5493.8317 | 1.3898 | Acc: 0.8184\n",
      "Epoch 00021: adjusting learning rate of group 0 to 1.9935e-01.\n",
      "e: 31 | Train Loss: 36126.5358 | 0.8063 | Acc: 0.8807\n",
      "e: 31 | Valid Loss: 5461.6754 | 1.3817 | Acc: 0.8189\n",
      "Validation loss decreased (5472.155046 --> 5461.675395).\n",
      "Epoch 00022: adjusting learning rate of group 0 to 1.9928e-01.\n",
      "e: 32 | Train Loss: 35663.5610 | 0.7960 | Acc: 0.8819\n",
      "e: 32 | Valid Loss: 5456.7591 | 1.3804 | Acc: 0.8196\n",
      "Validation loss decreased (5461.675395 --> 5456.759141).\n",
      "Epoch 00023: adjusting learning rate of group 0 to 1.9922e-01.\n",
      "e: 33 | Train Loss: 35142.1494 | 0.7843 | Acc: 0.8839\n",
      "e: 33 | Valid Loss: 5518.1399 | 1.3959 | Acc: 0.8199\n",
      "Epoch 00024: adjusting learning rate of group 0 to 1.9915e-01.\n",
      "e: 34 | Train Loss: 35575.6574 | 0.7940 | Acc: 0.8819\n",
      "e: 34 | Valid Loss: 5489.1932 | 1.3886 | Acc: 0.8166\n",
      "Epoch 00025: adjusting learning rate of group 0 to 1.9908e-01.\n",
      "e: 35 | Train Loss: 35493.1764 | 0.7922 | Acc: 0.8828\n",
      "e: 35 | Valid Loss: 5469.2632 | 1.3836 | Acc: 0.8211\n",
      "Epoch 00026: adjusting learning rate of group 0 to 1.9900e-01.\n",
      "e: 36 | Train Loss: 34783.6963 | 0.7763 | Acc: 0.8851\n",
      "e: 36 | Valid Loss: 5457.1533 | 1.3805 | Acc: 0.8189\n",
      "Epoch 00027: adjusting learning rate of group 0 to 1.9892e-01.\n",
      "e: 37 | Train Loss: 35332.5061 | 0.7886 | Acc: 0.8824\n",
      "e: 37 | Valid Loss: 5498.4298 | 1.3910 | Acc: 0.8206\n",
      "Epoch 00028: adjusting learning rate of group 0 to 1.9884e-01.\n",
      "e: 38 | Train Loss: 35256.9337 | 0.7869 | Acc: 0.8826\n",
      "e: 38 | Valid Loss: 5478.8635 | 1.3860 | Acc: 0.8194\n",
      "Epoch 00029: adjusting learning rate of group 0 to 1.9876e-01.\n",
      "e: 39 | Train Loss: 35126.0512 | 0.7840 | Acc: 0.8815\n",
      "e: 39 | Valid Loss: 5427.4178 | 1.3730 | Acc: 0.8176\n",
      "Validation loss decreased (5456.759141 --> 5427.417797).\n",
      "Epoch 00030: adjusting learning rate of group 0 to 1.9867e-01.\n",
      "e: 40 | Train Loss: 34233.7743 | 0.7641 | Acc: 0.8850\n",
      "e: 40 | Valid Loss: 5453.5520 | 1.3796 | Acc: 0.8176\n",
      "Epoch 00031: adjusting learning rate of group 0 to 1.9858e-01.\n",
      "e: 41 | Train Loss: 34989.6440 | 0.7809 | Acc: 0.8823\n",
      "e: 41 | Valid Loss: 5510.8336 | 1.3941 | Acc: 0.8186\n",
      "Epoch 00032: adjusting learning rate of group 0 to 1.9849e-01.\n",
      "e: 42 | Train Loss: 34336.6526 | 0.7664 | Acc: 0.8840\n",
      "e: 42 | Valid Loss: 5454.1735 | 1.3798 | Acc: 0.8163\n",
      "Epoch 00033: adjusting learning rate of group 0 to 1.9839e-01.\n",
      "e: 43 | Train Loss: 35111.9370 | 0.7837 | Acc: 0.8844\n",
      "e: 43 | Valid Loss: 5471.1554 | 1.3841 | Acc: 0.8189\n",
      "Epoch 00034: adjusting learning rate of group 0 to 1.9830e-01.\n",
      "e: 44 | Train Loss: 33902.9870 | 0.7567 | Acc: 0.8868\n",
      "e: 44 | Valid Loss: 5517.5667 | 1.3958 | Acc: 0.8176\n",
      "Epoch 00035: adjusting learning rate of group 0 to 1.9819e-01.\n",
      "e: 45 | Train Loss: 33961.1993 | 0.7580 | Acc: 0.8851\n",
      "e: 45 | Valid Loss: 5453.9192 | 1.3797 | Acc: 0.8209\n",
      "Epoch 00036: adjusting learning rate of group 0 to 1.9809e-01.\n",
      "e: 46 | Train Loss: 34149.8314 | 0.7622 | Acc: 0.8856\n",
      "e: 46 | Valid Loss: 5503.2574 | 1.3922 | Acc: 0.8217\n",
      "Epoch 00037: adjusting learning rate of group 0 to 1.9798e-01.\n",
      "e: 47 | Train Loss: 33744.6341 | 0.7531 | Acc: 0.8869\n",
      "e: 47 | Valid Loss: 5485.3014 | 1.3876 | Acc: 0.8191\n",
      "Epoch 00038: adjusting learning rate of group 0 to 1.9787e-01.\n",
      "e: 48 | Train Loss: 34475.7186 | 0.7695 | Acc: 0.8839\n",
      "e: 48 | Valid Loss: 5422.3890 | 1.3717 | Acc: 0.8217\n",
      "Validation loss decreased (5427.417797 --> 5422.388952).\n",
      "Epoch 00039: adjusting learning rate of group 0 to 1.9776e-01.\n",
      "e: 49 | Train Loss: 34356.8435 | 0.7668 | Acc: 0.8849\n",
      "e: 49 | Valid Loss: 5430.1145 | 1.3737 | Acc: 0.8191\n",
      "Epoch 00040: adjusting learning rate of group 0 to 1.9764e-01.\n",
      "e: 50 | Train Loss: 33124.4985 | 0.7393 | Acc: 0.8877\n",
      "e: 50 | Valid Loss: 5491.7187 | 1.3893 | Acc: 0.8184\n",
      "Epoch 00041: adjusting learning rate of group 0 to 1.9753e-01.\n",
      "e: 51 | Train Loss: 33062.2327 | 0.7379 | Acc: 0.8891\n",
      "e: 51 | Valid Loss: 5469.3049 | 1.3836 | Acc: 0.8194\n",
      "Epoch 00042: adjusting learning rate of group 0 to 1.9740e-01.\n",
      "e: 52 | Train Loss: 33057.0086 | 0.7378 | Acc: 0.8877\n",
      "e: 52 | Valid Loss: 5433.0660 | 1.3744 | Acc: 0.8191\n",
      "Epoch 00043: adjusting learning rate of group 0 to 1.9728e-01.\n",
      "e: 53 | Train Loss: 33314.5692 | 0.7435 | Acc: 0.8874\n",
      "e: 53 | Valid Loss: 5489.0731 | 1.3886 | Acc: 0.8194\n",
      "Epoch 00044: adjusting learning rate of group 0 to 1.9715e-01.\n",
      "e: 54 | Train Loss: 33153.9544 | 0.7400 | Acc: 0.8878\n",
      "e: 54 | Valid Loss: 5486.8947 | 1.3880 | Acc: 0.8191\n",
      "Epoch 00045: adjusting learning rate of group 0 to 1.9702e-01.\n",
      "e: 55 | Train Loss: 33375.0599 | 0.7449 | Acc: 0.8872\n",
      "e: 55 | Valid Loss: 5429.3476 | 1.3735 | Acc: 0.8194\n",
      "Epoch 00046: adjusting learning rate of group 0 to 1.9689e-01.\n",
      "e: 56 | Train Loss: 33060.0472 | 0.7379 | Acc: 0.8885\n",
      "e: 56 | Valid Loss: 5488.6061 | 1.3885 | Acc: 0.8189\n",
      "Epoch 00047: adjusting learning rate of group 0 to 1.9675e-01.\n",
      "e: 57 | Train Loss: 33607.6457 | 0.7501 | Acc: 0.8869\n",
      "e: 57 | Valid Loss: 5452.4501 | 1.3793 | Acc: 0.8194\n",
      "Epoch 00048: adjusting learning rate of group 0 to 1.9661e-01.\n",
      "e: 58 | Train Loss: 33016.7620 | 0.7369 | Acc: 0.8871\n",
      "e: 58 | Valid Loss: 5469.2855 | 1.3836 | Acc: 0.8214\n",
      "Epoch 00049: adjusting learning rate of group 0 to 1.9647e-01.\n",
      "e: 59 | Train Loss: 33343.4637 | 0.7442 | Acc: 0.8877\n",
      "e: 59 | Valid Loss: 5431.0662 | 1.3739 | Acc: 0.8184\n",
      "Epoch 00050: adjusting learning rate of group 0 to 1.9633e-01.\n",
      "e: 60 | Train Loss: 32716.4764 | 0.7302 | Acc: 0.8888\n",
      "e: 60 | Valid Loss: 5438.8228 | 1.3759 | Acc: 0.8217\n",
      "Epoch 00051: adjusting learning rate of group 0 to 1.9618e-01.\n",
      "e: 61 | Train Loss: 32646.2671 | 0.7286 | Acc: 0.8878\n",
      "e: 61 | Valid Loss: 5407.5777 | 1.3680 | Acc: 0.8209\n",
      "Validation loss decreased (5422.388952 --> 5407.577713).\n",
      "Epoch 00052: adjusting learning rate of group 0 to 1.9603e-01.\n",
      "e: 62 | Train Loss: 32557.9013 | 0.7267 | Acc: 0.8880\n",
      "e: 62 | Valid Loss: 5462.2290 | 1.3818 | Acc: 0.8214\n",
      "Epoch 00053: adjusting learning rate of group 0 to 1.9588e-01.\n",
      "e: 63 | Train Loss: 32639.2187 | 0.7285 | Acc: 0.8890\n",
      "e: 63 | Valid Loss: 5451.0583 | 1.3790 | Acc: 0.8199\n",
      "Epoch 00054: adjusting learning rate of group 0 to 1.9572e-01.\n",
      "e: 64 | Train Loss: 33448.5685 | 0.7465 | Acc: 0.8861\n",
      "e: 64 | Valid Loss: 5502.9380 | 1.3921 | Acc: 0.8181\n",
      "Epoch 00055: adjusting learning rate of group 0 to 1.9557e-01.\n",
      "e: 65 | Train Loss: 33011.6560 | 0.7368 | Acc: 0.8872\n",
      "e: 65 | Valid Loss: 5388.4829 | 1.3631 | Acc: 0.8196\n",
      "Validation loss decreased (5407.577713 --> 5388.482869).\n",
      "Epoch 00056: adjusting learning rate of group 0 to 1.9541e-01.\n",
      "e: 66 | Train Loss: 32751.3980 | 0.7310 | Acc: 0.8872\n",
      "e: 66 | Valid Loss: 5409.8113 | 1.3685 | Acc: 0.8214\n",
      "Epoch 00057: adjusting learning rate of group 0 to 1.9524e-01.\n",
      "e: 67 | Train Loss: 32389.4367 | 0.7229 | Acc: 0.8887\n",
      "e: 67 | Valid Loss: 5444.2712 | 1.3773 | Acc: 0.8199\n",
      "Epoch 00058: adjusting learning rate of group 0 to 1.9507e-01.\n",
      "e: 68 | Train Loss: 32554.5778 | 0.7266 | Acc: 0.8898\n",
      "e: 68 | Valid Loss: 5447.8052 | 1.3781 | Acc: 0.8196\n",
      "Epoch 00059: adjusting learning rate of group 0 to 1.9491e-01.\n",
      "e: 69 | Train Loss: 32473.3430 | 0.7248 | Acc: 0.8890\n",
      "e: 69 | Valid Loss: 5434.1047 | 1.3747 | Acc: 0.8206\n",
      "Epoch 00060: adjusting learning rate of group 0 to 1.9473e-01.\n",
      "e: 70 | Train Loss: 32403.5849 | 0.7232 | Acc: 0.8885\n",
      "e: 70 | Valid Loss: 5444.5390 | 1.3773 | Acc: 0.8217\n",
      "Epoch 00061: adjusting learning rate of group 0 to 1.9456e-01.\n",
      "e: 71 | Train Loss: 32937.0584 | 0.7351 | Acc: 0.8874\n",
      "e: 71 | Valid Loss: 5460.1604 | 1.3813 | Acc: 0.8194\n",
      "Epoch 00062: adjusting learning rate of group 0 to 1.9438e-01.\n",
      "e: 72 | Train Loss: 32656.2471 | 0.7289 | Acc: 0.8882\n",
      "e: 72 | Valid Loss: 5395.6065 | 1.3649 | Acc: 0.8196\n",
      "Epoch 00063: adjusting learning rate of group 0 to 1.9420e-01.\n",
      "e: 73 | Train Loss: 32005.0402 | 0.7143 | Acc: 0.8907\n",
      "e: 73 | Valid Loss: 5380.0456 | 1.3610 | Acc: 0.8227\n",
      "Validation loss decreased (5388.482869 --> 5380.045645).\n",
      "Epoch 00064: adjusting learning rate of group 0 to 1.9402e-01.\n",
      "e: 74 | Train Loss: 31742.8786 | 0.7085 | Acc: 0.8914\n",
      "e: 74 | Valid Loss: 5431.2535 | 1.3740 | Acc: 0.8219\n",
      "Epoch 00065: adjusting learning rate of group 0 to 1.9383e-01.\n",
      "e: 75 | Train Loss: 31805.3122 | 0.7099 | Acc: 0.8915\n",
      "e: 75 | Valid Loss: 5445.9576 | 1.3777 | Acc: 0.8206\n",
      "Epoch 00066: adjusting learning rate of group 0 to 1.9364e-01.\n",
      "e: 76 | Train Loss: 31967.0764 | 0.7135 | Acc: 0.8919\n",
      "e: 76 | Valid Loss: 5457.4429 | 1.3806 | Acc: 0.8189\n",
      "Epoch 00067: adjusting learning rate of group 0 to 1.9345e-01.\n",
      "e: 77 | Train Loss: 32186.6602 | 0.7184 | Acc: 0.8892\n",
      "e: 77 | Valid Loss: 5430.6961 | 1.3738 | Acc: 0.8219\n",
      "Epoch 00068: adjusting learning rate of group 0 to 1.9326e-01.\n",
      "e: 78 | Train Loss: 32457.5881 | 0.7244 | Acc: 0.8891\n",
      "e: 78 | Valid Loss: 5428.1914 | 1.3732 | Acc: 0.8199\n",
      "Epoch 00069: adjusting learning rate of group 0 to 1.9306e-01.\n",
      "e: 79 | Train Loss: 31425.6607 | 0.7014 | Acc: 0.8935\n",
      "e: 79 | Valid Loss: 5472.2474 | 1.3843 | Acc: 0.8211\n",
      "Epoch 00070: adjusting learning rate of group 0 to 1.9286e-01.\n",
      "e: 80 | Train Loss: 31956.5479 | 0.7132 | Acc: 0.8897\n",
      "e: 80 | Valid Loss: 5459.9203 | 1.3812 | Acc: 0.8217\n",
      "Epoch 00071: adjusting learning rate of group 0 to 1.9266e-01.\n",
      "e: 81 | Train Loss: 32249.0236 | 0.7198 | Acc: 0.8894\n",
      "e: 81 | Valid Loss: 5416.5936 | 1.3702 | Acc: 0.8196\n",
      "Epoch 00072: adjusting learning rate of group 0 to 1.9246e-01.\n",
      "e: 82 | Train Loss: 32297.1764 | 0.7208 | Acc: 0.8882\n",
      "e: 82 | Valid Loss: 5464.8402 | 1.3825 | Acc: 0.8214\n",
      "Epoch 00073: adjusting learning rate of group 0 to 1.9225e-01.\n",
      "e: 83 | Train Loss: 31774.8132 | 0.7092 | Acc: 0.8920\n",
      "e: 83 | Valid Loss: 5424.8174 | 1.3723 | Acc: 0.8201\n",
      "Epoch 00074: adjusting learning rate of group 0 to 1.9204e-01.\n",
      "e: 84 | Train Loss: 32303.2059 | 0.7210 | Acc: 0.8896\n",
      "e: 84 | Valid Loss: 5467.3961 | 1.3831 | Acc: 0.8184\n",
      "Epoch 00075: adjusting learning rate of group 0 to 1.9183e-01.\n",
      "e: 85 | Train Loss: 31907.6134 | 0.7121 | Acc: 0.8918\n",
      "e: 85 | Valid Loss: 5418.9053 | 1.3708 | Acc: 0.8229\n",
      "Epoch 00076: adjusting learning rate of group 0 to 1.9161e-01.\n",
      "e: 86 | Train Loss: 31756.1763 | 0.7088 | Acc: 0.8921\n",
      "e: 86 | Valid Loss: 5443.6665 | 1.3771 | Acc: 0.8219\n",
      "Epoch 00077: adjusting learning rate of group 0 to 1.9139e-01.\n",
      "e: 87 | Train Loss: 31299.1175 | 0.6986 | Acc: 0.8940\n",
      "e: 87 | Valid Loss: 5468.0016 | 1.3833 | Acc: 0.8206\n",
      "Epoch 00078: adjusting learning rate of group 0 to 1.9117e-01.\n",
      "e: 88 | Train Loss: 31471.5033 | 0.7024 | Acc: 0.8919\n",
      "e: 88 | Valid Loss: 5515.5088 | 1.3953 | Acc: 0.8189\n",
      "Epoch 00079: adjusting learning rate of group 0 to 1.9095e-01.\n",
      "e: 89 | Train Loss: 31471.8404 | 0.7024 | Acc: 0.8908\n",
      "e: 89 | Valid Loss: 5446.7310 | 1.3779 | Acc: 0.8217\n",
      "Epoch 00080: adjusting learning rate of group 0 to 1.9072e-01.\n",
      "e: 90 | Train Loss: 32149.1108 | 0.7175 | Acc: 0.8898\n",
      "e: 90 | Valid Loss: 5435.3195 | 1.3750 | Acc: 0.8217\n",
      "Epoch 00081: adjusting learning rate of group 0 to 1.9049e-01.\n",
      "e: 91 | Train Loss: 32018.9003 | 0.7146 | Acc: 0.8896\n",
      "e: 91 | Valid Loss: 5516.7088 | 1.3956 | Acc: 0.8204\n",
      "Epoch 00082: adjusting learning rate of group 0 to 1.9026e-01.\n",
      "e: 92 | Train Loss: 31559.9497 | 0.7044 | Acc: 0.8915\n",
      "e: 92 | Valid Loss: 5506.1394 | 1.3929 | Acc: 0.8211\n",
      "Epoch 00083: adjusting learning rate of group 0 to 1.9003e-01.\n",
      "e: 93 | Train Loss: 31190.6526 | 0.6961 | Acc: 0.8914\n",
      "e: 93 | Valid Loss: 5506.2696 | 1.3929 | Acc: 0.8191\n",
      "Epoch 00084: adjusting learning rate of group 0 to 1.8979e-01.\n",
      "e: 94 | Train Loss: 30693.3847 | 0.6850 | Acc: 0.8939\n",
      "e: 94 | Valid Loss: 5438.4612 | 1.3758 | Acc: 0.8244\n",
      "Epoch 00085: adjusting learning rate of group 0 to 1.8956e-01.\n",
      "e: 95 | Train Loss: 31304.2058 | 0.6987 | Acc: 0.8925\n",
      "e: 95 | Valid Loss: 5463.1999 | 1.3820 | Acc: 0.8191\n",
      "Epoch 00086: adjusting learning rate of group 0 to 1.8931e-01.\n",
      "e: 96 | Train Loss: 31429.7898 | 0.7015 | Acc: 0.8920\n",
      "e: 96 | Valid Loss: 5446.2125 | 1.3777 | Acc: 0.8237\n",
      "Epoch 00087: adjusting learning rate of group 0 to 1.8907e-01.\n",
      "e: 97 | Train Loss: 30870.7113 | 0.6890 | Acc: 0.8933\n",
      "e: 97 | Valid Loss: 5505.8516 | 1.3928 | Acc: 0.8201\n",
      "Epoch 00088: adjusting learning rate of group 0 to 1.8882e-01.\n",
      "e: 98 | Train Loss: 31498.3349 | 0.7030 | Acc: 0.8920\n",
      "e: 98 | Valid Loss: 5499.2919 | 1.3912 | Acc: 0.8209\n",
      "Epoch 00089: adjusting learning rate of group 0 to 1.8858e-01.\n",
      "e: 99 | Train Loss: 31057.2671 | 0.6932 | Acc: 0.8914\n",
      "e: 99 | Valid Loss: 5487.1413 | 1.3881 | Acc: 0.8206\n",
      "Epoch 00090: adjusting learning rate of group 0 to 1.8832e-01.\n",
      "e: 100 | Train Loss: 31622.7198 | 0.7058 | Acc: 0.8906\n",
      "e: 100 | Valid Loss: 5400.5940 | 1.3662 | Acc: 0.8227\n",
      "Epoch 00091: adjusting learning rate of group 0 to 1.8807e-01.\n",
      "e: 101 | Train Loss: 31223.9920 | 0.6969 | Acc: 0.8926\n",
      "e: 101 | Valid Loss: 5413.8549 | 1.3696 | Acc: 0.8194\n",
      "Test Loss: 5335.8047 | 1.3495 | Acc: 0.8214\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch_acc</td><td>▁▅▆▇▇▇▇▇▇███████████████████████████████</td></tr><tr><td>epoch_loss</td><td>█▅▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>running_loss</td><td>█▅▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>test_running_loss</td><td>▁</td></tr><tr><td>val_acc</td><td>▁▃▅▆▇▇▇█████████████████████████████████</td></tr><tr><td>val_loss</td><td>█▆▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_running_loss</td><td>█▆▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>101</td></tr><tr><td>epoch_acc</td><td>0.89265</td></tr><tr><td>epoch_loss</td><td>0.69689</td></tr><tr><td>running_loss</td><td>31223.99204</td></tr><tr><td>test_acc</td><td>0.82145</td></tr><tr><td>test_loss</td><td>1.34947</td></tr><tr><td>test_running_loss</td><td>5335.80471</td></tr><tr><td>val_acc</td><td>0.81938</td></tr><tr><td>val_loss</td><td>1.36956</td></tr><tr><td>val_running_loss</td><td>5413.85486</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">ResNet50-HIE-33-400-linear</strong>: <a href=\"https://wandb.ai/33h002/DeepFashion-pre-linear/runs/1ijdktwn\" target=\"_blank\">https://wandb.ai/33h002/DeepFashion-pre-linear/runs/1ijdktwn</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221201_130801-1ijdktwn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "from src.train import run_linearprotocol, run_test\n",
    "\n",
    "\n",
    "wandb.init(name=RUN_NAME, project=WANDB_PRJ, config=CONFIG, reinit=True)\n",
    "\n",
    "run_linearprotocol(model, mlp, dataloaders, dataset_sizes,\n",
    "                   optimizer, criterion, warmupscheduler, mainscheduler, device, RUN_NAME, SAVE_PATH)\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = timm.create_model('resnet50', pretrained=False, num_classes=num_classes)\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "\n",
    "mlp = MLP(num_classes=17)\n",
    "mlp.load_state_dict(torch.load(f'{SAVE_PATH}/{RUN_NAME}-MLP-best_model.pth'))  \n",
    "\n",
    "run_test(model, mlp, dataloaders, dataset_sizes, criterion, device)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80ff78b-2294-4248-ad1d-1e8939716cb7",
   "metadata": {},
   "source": [
    "-------\n",
    "### SupCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "637b7a96-aee8-499f-8b16-0ee0d656f269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda:0 - DeepFashion-pre-linear / ResNet50-SupCL-33-400-linear\n"
     ]
    }
   ],
   "source": [
    "SEED = 333\n",
    "seed_everything(SEED)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "MODEL_PATH = '../../../pretrained/ResNet50-SupCL-33+300-400.pth'\n",
    "SAVE_PATH = '../../../weights'\n",
    "\n",
    "RUN_NAME = 'ResNet50-SupCL-33-400-linear'\n",
    "WANDB_PRJ = 'DeepFashion-pre-linear'\n",
    "\n",
    "CONFIG = {\n",
    "    \"dataset\": \"DeepFashion\",\n",
    "    \"machine\": \"offline cluster\",\n",
    "    \"model\": \"ResNet50-SupCL\",\n",
    "    \"learning_rate\": 0.2,\n",
    "    \"batch_size\": batch_size*2\n",
    "}\n",
    "\n",
    "print(f'Training on {device} - {WANDB_PRJ} / {RUN_NAME}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e188231-2acf-45b5-99fa-f06f4c6df8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 2.0000e-02.\n",
      "Epoch 00000: adjusting learning rate of group 0 to 2.0000e-01.\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from src.models import MLP\n",
    "from src.optim_lars import LARS\n",
    "\n",
    "num_classes = 0\n",
    "model = timm.create_model('resnet50', pretrained=False, num_classes=num_classes)\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "\n",
    "mlp = MLP(num_classes=17) \n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = LARS(\n",
    "    list(mlp.parameters()),\n",
    "    lr=0.2,\n",
    "    weight_decay=1e-6,\n",
    "    exclude_from_weight_decay=['batch_normalization', 'bias'],\n",
    ")\n",
    "\n",
    "warmupscheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch : (epoch+1)/10.0, verbose = True)\n",
    "mainscheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 500, eta_min=0.05, last_epoch=-1, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbc04a6a-b91a-43cd-a355-d47c3499023b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/soohee/Workspace/cv/git/MLDL-Prj/ImgR/wandb/run-20221201_184644-2f1oqot8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/33h002/DeepFashion-pre-linear/runs/2f1oqot8\" target=\"_blank\">ResNet50-SupCL-33-400-linear</a></strong> to <a href=\"https://wandb.ai/33h002/DeepFashion-pre-linear\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 4.0000e-02.\n",
      "e: 1 | Train Loss: 89719.7514 | 2.0024 | Acc: 0.8130\n",
      "e: 1 | Valid Loss: 6538.3100 | 1.6540 | Acc: 0.8174\n",
      "Validation loss decreased (inf --> 6538.309963).\n",
      "Adjusting learning rate of group 0 to 6.0000e-02.\n",
      "e: 2 | Train Loss: 47838.4621 | 1.0677 | Acc: 0.8668\n",
      "e: 2 | Valid Loss: 6519.8922 | 1.6494 | Acc: 0.8158\n",
      "Validation loss decreased (6538.309963 --> 6519.892226).\n",
      "Adjusting learning rate of group 0 to 8.0000e-02.\n",
      "e: 3 | Train Loss: 46777.3448 | 1.0440 | Acc: 0.8657\n",
      "e: 3 | Valid Loss: 6503.4688 | 1.6452 | Acc: 0.8158\n",
      "Validation loss decreased (6519.892226 --> 6503.468764).\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "e: 4 | Train Loss: 45517.6220 | 1.0159 | Acc: 0.8662\n",
      "e: 4 | Valid Loss: 6511.9045 | 1.6473 | Acc: 0.8168\n",
      "Adjusting learning rate of group 0 to 1.2000e-01.\n",
      "e: 5 | Train Loss: 43960.4647 | 0.9812 | Acc: 0.8701\n",
      "e: 5 | Valid Loss: 6515.3044 | 1.6482 | Acc: 0.8179\n",
      "Adjusting learning rate of group 0 to 1.4000e-01.\n",
      "e: 6 | Train Loss: 44010.3588 | 0.9823 | Acc: 0.8678\n",
      "e: 6 | Valid Loss: 6464.2099 | 1.6353 | Acc: 0.8171\n",
      "Validation loss decreased (6503.468764 --> 6464.209870).\n",
      "Adjusting learning rate of group 0 to 1.6000e-01.\n",
      "e: 7 | Train Loss: 42438.0243 | 0.9472 | Acc: 0.8729\n",
      "e: 7 | Valid Loss: 6446.2591 | 1.6307 | Acc: 0.8156\n",
      "Validation loss decreased (6464.209870 --> 6446.259110).\n",
      "Adjusting learning rate of group 0 to 1.8000e-01.\n",
      "e: 8 | Train Loss: 42616.9041 | 0.9512 | Acc: 0.8710\n",
      "e: 8 | Valid Loss: 6456.8365 | 1.6334 | Acc: 0.8161\n",
      "Adjusting learning rate of group 0 to 2.0000e-01.\n",
      "e: 9 | Train Loss: 42415.0502 | 0.9467 | Acc: 0.8740\n",
      "e: 9 | Valid Loss: 6409.1973 | 1.6214 | Acc: 0.8163\n",
      "Validation loss decreased (6446.259110 --> 6409.197336).\n",
      "Adjusting learning rate of group 0 to 2.2000e-01.\n",
      "e: 10 | Train Loss: 41078.4986 | 0.9168 | Acc: 0.8753\n",
      "e: 10 | Valid Loss: 6401.4911 | 1.6194 | Acc: 0.8171\n",
      "Validation loss decreased (6409.197336 --> 6401.491131).\n",
      "Epoch 00001: adjusting learning rate of group 0 to 2.0000e-01.\n",
      "e: 11 | Train Loss: 41186.4444 | 0.9192 | Acc: 0.8737\n",
      "e: 11 | Valid Loss: 6428.2612 | 1.6262 | Acc: 0.8151\n",
      "Epoch 00002: adjusting learning rate of group 0 to 1.9999e-01.\n",
      "e: 12 | Train Loss: 41287.4023 | 0.9215 | Acc: 0.8723\n",
      "e: 12 | Valid Loss: 6355.9060 | 1.6079 | Acc: 0.8161\n",
      "Validation loss decreased (6401.491131 --> 6355.906020).\n",
      "Epoch 00003: adjusting learning rate of group 0 to 1.9999e-01.\n",
      "e: 13 | Train Loss: 40347.3571 | 0.9005 | Acc: 0.8748\n",
      "e: 13 | Valid Loss: 6469.5066 | 1.6366 | Acc: 0.8158\n",
      "Epoch 00004: adjusting learning rate of group 0 to 1.9998e-01.\n",
      "e: 14 | Train Loss: 40547.6727 | 0.9050 | Acc: 0.8750\n",
      "e: 14 | Valid Loss: 6401.5464 | 1.6194 | Acc: 0.8161\n",
      "Epoch 00005: adjusting learning rate of group 0 to 1.9996e-01.\n",
      "e: 15 | Train Loss: 39925.4250 | 0.8911 | Acc: 0.8767\n",
      "e: 15 | Valid Loss: 6293.7935 | 1.5922 | Acc: 0.8161\n",
      "Validation loss decreased (6355.906020 --> 6293.793550).\n",
      "Epoch 00006: adjusting learning rate of group 0 to 1.9995e-01.\n",
      "e: 16 | Train Loss: 40339.8575 | 0.9003 | Acc: 0.8741\n",
      "e: 16 | Valid Loss: 6360.4664 | 1.6090 | Acc: 0.8156\n",
      "Epoch 00007: adjusting learning rate of group 0 to 1.9993e-01.\n",
      "e: 17 | Train Loss: 40111.1919 | 0.8952 | Acc: 0.8761\n",
      "e: 17 | Valid Loss: 6325.2347 | 1.6001 | Acc: 0.8161\n",
      "Epoch 00008: adjusting learning rate of group 0 to 1.9991e-01.\n",
      "e: 18 | Train Loss: 40321.7559 | 0.8999 | Acc: 0.8754\n",
      "e: 18 | Valid Loss: 6299.8078 | 1.5937 | Acc: 0.8156\n",
      "Epoch 00009: adjusting learning rate of group 0 to 1.9988e-01.\n",
      "e: 19 | Train Loss: 39565.3243 | 0.8831 | Acc: 0.8758\n",
      "e: 19 | Valid Loss: 6303.0633 | 1.5945 | Acc: 0.8151\n",
      "Epoch 00010: adjusting learning rate of group 0 to 1.9985e-01.\n",
      "e: 20 | Train Loss: 39991.9484 | 0.8926 | Acc: 0.8762\n",
      "e: 20 | Valid Loss: 6331.8177 | 1.6018 | Acc: 0.8158\n",
      "Epoch 00011: adjusting learning rate of group 0 to 1.9982e-01.\n",
      "e: 21 | Train Loss: 39519.4857 | 0.8820 | Acc: 0.8757\n",
      "e: 21 | Valid Loss: 6346.2760 | 1.6054 | Acc: 0.8158\n",
      "Epoch 00012: adjusting learning rate of group 0 to 1.9979e-01.\n",
      "e: 22 | Train Loss: 39305.1174 | 0.8772 | Acc: 0.8762\n",
      "e: 22 | Valid Loss: 6302.3983 | 1.5943 | Acc: 0.8153\n",
      "Epoch 00013: adjusting learning rate of group 0 to 1.9975e-01.\n",
      "e: 23 | Train Loss: 39026.4531 | 0.8710 | Acc: 0.8769\n",
      "e: 23 | Valid Loss: 6331.8015 | 1.6018 | Acc: 0.8166\n",
      "Epoch 00014: adjusting learning rate of group 0 to 1.9971e-01.\n",
      "e: 24 | Train Loss: 38848.2838 | 0.8671 | Acc: 0.8778\n",
      "e: 24 | Valid Loss: 6246.8847 | 1.5803 | Acc: 0.8163\n",
      "Validation loss decreased (6293.793550 --> 6246.884695).\n",
      "Epoch 00015: adjusting learning rate of group 0 to 1.9967e-01.\n",
      "e: 25 | Train Loss: 38696.6280 | 0.8637 | Acc: 0.8794\n",
      "e: 25 | Valid Loss: 6321.8466 | 1.5993 | Acc: 0.8151\n",
      "Epoch 00016: adjusting learning rate of group 0 to 1.9962e-01.\n",
      "e: 26 | Train Loss: 38771.7689 | 0.8653 | Acc: 0.8796\n",
      "e: 26 | Valid Loss: 6322.5058 | 1.5994 | Acc: 0.8168\n",
      "Epoch 00017: adjusting learning rate of group 0 to 1.9957e-01.\n",
      "e: 27 | Train Loss: 38252.5605 | 0.8538 | Acc: 0.8791\n",
      "e: 27 | Valid Loss: 6259.7959 | 1.5836 | Acc: 0.8156\n",
      "Epoch 00018: adjusting learning rate of group 0 to 1.9952e-01.\n",
      "e: 28 | Train Loss: 38051.9708 | 0.8493 | Acc: 0.8809\n",
      "e: 28 | Valid Loss: 6329.8616 | 1.6013 | Acc: 0.8148\n",
      "Epoch 00019: adjusting learning rate of group 0 to 1.9947e-01.\n",
      "e: 29 | Train Loss: 39199.5991 | 0.8749 | Acc: 0.8768\n",
      "e: 29 | Valid Loss: 6222.9530 | 1.5742 | Acc: 0.8163\n",
      "Validation loss decreased (6246.884695 --> 6222.952981).\n",
      "Epoch 00020: adjusting learning rate of group 0 to 1.9941e-01.\n",
      "e: 30 | Train Loss: 38419.4098 | 0.8575 | Acc: 0.8786\n",
      "e: 30 | Valid Loss: 6222.4223 | 1.5741 | Acc: 0.8151\n",
      "Validation loss decreased (6222.952981 --> 6222.422291).\n",
      "Epoch 00021: adjusting learning rate of group 0 to 1.9935e-01.\n",
      "e: 31 | Train Loss: 37755.0901 | 0.8427 | Acc: 0.8805\n",
      "e: 31 | Valid Loss: 6281.2734 | 1.5890 | Acc: 0.8156\n",
      "Epoch 00022: adjusting learning rate of group 0 to 1.9928e-01.\n",
      "e: 32 | Train Loss: 37568.2324 | 0.8385 | Acc: 0.8801\n",
      "e: 32 | Valid Loss: 6278.9600 | 1.5884 | Acc: 0.8156\n",
      "Epoch 00023: adjusting learning rate of group 0 to 1.9922e-01.\n",
      "e: 33 | Train Loss: 37923.6204 | 0.8464 | Acc: 0.8793\n",
      "e: 33 | Valid Loss: 6217.4065 | 1.5728 | Acc: 0.8146\n",
      "Validation loss decreased (6222.422291 --> 6217.406469).\n",
      "Epoch 00024: adjusting learning rate of group 0 to 1.9915e-01.\n",
      "e: 34 | Train Loss: 38158.0194 | 0.8516 | Acc: 0.8787\n",
      "e: 34 | Valid Loss: 6164.4316 | 1.5594 | Acc: 0.8161\n",
      "Validation loss decreased (6217.406469 --> 6164.431590).\n",
      "Epoch 00025: adjusting learning rate of group 0 to 1.9908e-01.\n",
      "e: 35 | Train Loss: 37485.1414 | 0.8366 | Acc: 0.8801\n",
      "e: 35 | Valid Loss: 6200.1583 | 1.5685 | Acc: 0.8146\n",
      "Epoch 00026: adjusting learning rate of group 0 to 1.9900e-01.\n",
      "e: 36 | Train Loss: 37245.8529 | 0.8313 | Acc: 0.8816\n",
      "e: 36 | Valid Loss: 6254.7824 | 1.5823 | Acc: 0.8143\n",
      "Epoch 00027: adjusting learning rate of group 0 to 1.9892e-01.\n",
      "e: 37 | Train Loss: 38109.5026 | 0.8506 | Acc: 0.8793\n",
      "e: 37 | Valid Loss: 6097.2610 | 1.5424 | Acc: 0.8148\n",
      "Validation loss decreased (6164.431590 --> 6097.261003).\n",
      "Epoch 00028: adjusting learning rate of group 0 to 1.9884e-01.\n",
      "e: 38 | Train Loss: 36909.5500 | 0.8238 | Acc: 0.8828\n",
      "e: 38 | Valid Loss: 6263.4162 | 1.5845 | Acc: 0.8143\n",
      "Epoch 00029: adjusting learning rate of group 0 to 1.9876e-01.\n",
      "e: 39 | Train Loss: 37588.6714 | 0.8389 | Acc: 0.8790\n",
      "e: 39 | Valid Loss: 6174.9717 | 1.5621 | Acc: 0.8148\n",
      "Epoch 00030: adjusting learning rate of group 0 to 1.9867e-01.\n",
      "e: 40 | Train Loss: 36716.2845 | 0.8195 | Acc: 0.8831\n",
      "e: 40 | Valid Loss: 6278.1544 | 1.5882 | Acc: 0.8158\n",
      "Epoch 00031: adjusting learning rate of group 0 to 1.9858e-01.\n",
      "e: 41 | Train Loss: 37109.2237 | 0.8282 | Acc: 0.8824\n",
      "e: 41 | Valid Loss: 6311.3397 | 1.5966 | Acc: 0.8151\n",
      "Epoch 00032: adjusting learning rate of group 0 to 1.9849e-01.\n",
      "e: 42 | Train Loss: 36709.0990 | 0.8193 | Acc: 0.8818\n",
      "e: 42 | Valid Loss: 6155.0929 | 1.5571 | Acc: 0.8158\n",
      "Epoch 00033: adjusting learning rate of group 0 to 1.9839e-01.\n",
      "e: 43 | Train Loss: 37135.0301 | 0.8288 | Acc: 0.8821\n",
      "e: 43 | Valid Loss: 6181.3698 | 1.5637 | Acc: 0.8168\n",
      "Epoch 00034: adjusting learning rate of group 0 to 1.9830e-01.\n",
      "e: 44 | Train Loss: 36470.1844 | 0.8140 | Acc: 0.8835\n",
      "e: 44 | Valid Loss: 6240.2034 | 1.5786 | Acc: 0.8156\n",
      "Epoch 00035: adjusting learning rate of group 0 to 1.9819e-01.\n",
      "e: 45 | Train Loss: 36539.6861 | 0.8155 | Acc: 0.8841\n",
      "e: 45 | Valid Loss: 6240.0761 | 1.5786 | Acc: 0.8163\n",
      "Epoch 00036: adjusting learning rate of group 0 to 1.9809e-01.\n",
      "e: 46 | Train Loss: 36774.5355 | 0.8208 | Acc: 0.8824\n",
      "e: 46 | Valid Loss: 6144.5500 | 1.5544 | Acc: 0.8146\n",
      "Epoch 00037: adjusting learning rate of group 0 to 1.9798e-01.\n",
      "e: 47 | Train Loss: 36070.0868 | 0.8050 | Acc: 0.8849\n",
      "e: 47 | Valid Loss: 6206.7124 | 1.5701 | Acc: 0.8148\n",
      "Epoch 00038: adjusting learning rate of group 0 to 1.9787e-01.\n",
      "e: 48 | Train Loss: 36703.6011 | 0.8192 | Acc: 0.8835\n",
      "e: 48 | Valid Loss: 6190.8481 | 1.5661 | Acc: 0.8158\n",
      "Epoch 00039: adjusting learning rate of group 0 to 1.9776e-01.\n",
      "e: 49 | Train Loss: 36334.2844 | 0.8109 | Acc: 0.8831\n",
      "e: 49 | Valid Loss: 6210.1031 | 1.5710 | Acc: 0.8166\n",
      "Epoch 00040: adjusting learning rate of group 0 to 1.9764e-01.\n",
      "e: 50 | Train Loss: 35658.4747 | 0.7959 | Acc: 0.8866\n",
      "e: 50 | Valid Loss: 6162.2624 | 1.5589 | Acc: 0.8171\n",
      "Epoch 00041: adjusting learning rate of group 0 to 1.9753e-01.\n",
      "e: 51 | Train Loss: 35573.2262 | 0.7940 | Acc: 0.8861\n",
      "e: 51 | Valid Loss: 6197.9448 | 1.5679 | Acc: 0.8153\n",
      "Epoch 00042: adjusting learning rate of group 0 to 1.9740e-01.\n",
      "e: 52 | Train Loss: 35672.0412 | 0.7962 | Acc: 0.8843\n",
      "e: 52 | Valid Loss: 6246.2256 | 1.5801 | Acc: 0.8174\n",
      "Epoch 00043: adjusting learning rate of group 0 to 1.9728e-01.\n",
      "e: 53 | Train Loss: 35180.6725 | 0.7852 | Acc: 0.8854\n",
      "e: 53 | Valid Loss: 6219.8342 | 1.5734 | Acc: 0.8174\n",
      "Epoch 00044: adjusting learning rate of group 0 to 1.9715e-01.\n",
      "e: 54 | Train Loss: 36078.8175 | 0.8052 | Acc: 0.8842\n",
      "e: 54 | Valid Loss: 6203.8948 | 1.5694 | Acc: 0.8161\n",
      "Epoch 00045: adjusting learning rate of group 0 to 1.9702e-01.\n",
      "e: 55 | Train Loss: 35955.3560 | 0.8025 | Acc: 0.8846\n",
      "e: 55 | Valid Loss: 6092.8132 | 1.5413 | Acc: 0.8161\n",
      "Validation loss decreased (6097.261003 --> 6092.813250).\n",
      "Epoch 00046: adjusting learning rate of group 0 to 1.9689e-01.\n",
      "e: 56 | Train Loss: 35744.3490 | 0.7978 | Acc: 0.8860\n",
      "e: 56 | Valid Loss: 6183.0147 | 1.5641 | Acc: 0.8171\n",
      "Epoch 00047: adjusting learning rate of group 0 to 1.9675e-01.\n",
      "e: 57 | Train Loss: 35965.4431 | 0.8027 | Acc: 0.8852\n",
      "e: 57 | Valid Loss: 6112.3971 | 1.5463 | Acc: 0.8161\n",
      "Epoch 00048: adjusting learning rate of group 0 to 1.9661e-01.\n",
      "e: 58 | Train Loss: 35603.3649 | 0.7946 | Acc: 0.8841\n",
      "e: 58 | Valid Loss: 6134.5895 | 1.5519 | Acc: 0.8168\n",
      "Epoch 00049: adjusting learning rate of group 0 to 1.9647e-01.\n",
      "e: 59 | Train Loss: 35390.9055 | 0.7899 | Acc: 0.8847\n",
      "e: 59 | Valid Loss: 6149.4711 | 1.5556 | Acc: 0.8168\n",
      "Epoch 00050: adjusting learning rate of group 0 to 1.9633e-01.\n",
      "e: 60 | Train Loss: 35013.1281 | 0.7815 | Acc: 0.8868\n",
      "e: 60 | Valid Loss: 6130.6843 | 1.5509 | Acc: 0.8171\n",
      "Epoch 00051: adjusting learning rate of group 0 to 1.9618e-01.\n",
      "e: 61 | Train Loss: 34912.7211 | 0.7792 | Acc: 0.8871\n",
      "e: 61 | Valid Loss: 6168.8277 | 1.5605 | Acc: 0.8153\n",
      "Epoch 00052: adjusting learning rate of group 0 to 1.9603e-01.\n",
      "e: 62 | Train Loss: 35408.6888 | 0.7903 | Acc: 0.8867\n",
      "e: 62 | Valid Loss: 6232.3904 | 1.5766 | Acc: 0.8174\n",
      "Epoch 00053: adjusting learning rate of group 0 to 1.9588e-01.\n",
      "e: 63 | Train Loss: 35250.0044 | 0.7867 | Acc: 0.8864\n",
      "e: 63 | Valid Loss: 6129.6567 | 1.5506 | Acc: 0.8168\n",
      "Epoch 00054: adjusting learning rate of group 0 to 1.9572e-01.\n",
      "e: 64 | Train Loss: 35949.7222 | 0.8024 | Acc: 0.8834\n",
      "e: 64 | Valid Loss: 6115.0975 | 1.5470 | Acc: 0.8184\n",
      "Epoch 00055: adjusting learning rate of group 0 to 1.9557e-01.\n",
      "e: 65 | Train Loss: 35130.1352 | 0.7841 | Acc: 0.8861\n",
      "e: 65 | Valid Loss: 6228.9083 | 1.5757 | Acc: 0.8158\n",
      "Epoch 00056: adjusting learning rate of group 0 to 1.9541e-01.\n",
      "e: 66 | Train Loss: 35376.0190 | 0.7896 | Acc: 0.8838\n",
      "e: 66 | Valid Loss: 6204.6435 | 1.5696 | Acc: 0.8158\n",
      "Epoch 00057: adjusting learning rate of group 0 to 1.9524e-01.\n",
      "e: 67 | Train Loss: 34807.5741 | 0.7769 | Acc: 0.8862\n",
      "e: 67 | Valid Loss: 6202.5815 | 1.5691 | Acc: 0.8168\n",
      "Epoch 00058: adjusting learning rate of group 0 to 1.9507e-01.\n",
      "e: 68 | Train Loss: 35573.7334 | 0.7940 | Acc: 0.8839\n",
      "e: 68 | Valid Loss: 6014.8387 | 1.5216 | Acc: 0.8156\n",
      "Validation loss decreased (6092.813250 --> 6014.838707).\n",
      "Epoch 00059: adjusting learning rate of group 0 to 1.9491e-01.\n",
      "e: 69 | Train Loss: 34795.5456 | 0.7766 | Acc: 0.8869\n",
      "e: 69 | Valid Loss: 6308.7555 | 1.5959 | Acc: 0.8156\n",
      "Epoch 00060: adjusting learning rate of group 0 to 1.9473e-01.\n",
      "e: 70 | Train Loss: 34984.5444 | 0.7808 | Acc: 0.8858\n",
      "e: 70 | Valid Loss: 6122.5056 | 1.5488 | Acc: 0.8161\n",
      "Epoch 00061: adjusting learning rate of group 0 to 1.9456e-01.\n",
      "e: 71 | Train Loss: 35516.2240 | 0.7927 | Acc: 0.8841\n",
      "e: 71 | Valid Loss: 6199.8801 | 1.5684 | Acc: 0.8153\n",
      "Epoch 00062: adjusting learning rate of group 0 to 1.9438e-01.\n",
      "e: 72 | Train Loss: 34798.9196 | 0.7767 | Acc: 0.8874\n",
      "e: 72 | Valid Loss: 6093.8424 | 1.5416 | Acc: 0.8156\n",
      "Epoch 00063: adjusting learning rate of group 0 to 1.9420e-01.\n",
      "e: 73 | Train Loss: 34489.4908 | 0.7698 | Acc: 0.8868\n",
      "e: 73 | Valid Loss: 6108.1294 | 1.5452 | Acc: 0.8168\n",
      "Epoch 00064: adjusting learning rate of group 0 to 1.9402e-01.\n",
      "e: 74 | Train Loss: 34207.5783 | 0.7635 | Acc: 0.8890\n",
      "e: 74 | Valid Loss: 6093.3584 | 1.5415 | Acc: 0.8179\n",
      "Epoch 00065: adjusting learning rate of group 0 to 1.9383e-01.\n",
      "e: 75 | Train Loss: 34763.2948 | 0.7759 | Acc: 0.8868\n",
      "e: 75 | Valid Loss: 6284.5149 | 1.5898 | Acc: 0.8171\n",
      "Epoch 00066: adjusting learning rate of group 0 to 1.9364e-01.\n",
      "e: 76 | Train Loss: 34505.0798 | 0.7701 | Acc: 0.8875\n",
      "e: 76 | Valid Loss: 6280.8508 | 1.5889 | Acc: 0.8184\n",
      "Epoch 00067: adjusting learning rate of group 0 to 1.9345e-01.\n",
      "e: 77 | Train Loss: 34836.1430 | 0.7775 | Acc: 0.8858\n",
      "e: 77 | Valid Loss: 6180.5926 | 1.5635 | Acc: 0.8166\n",
      "Epoch 00068: adjusting learning rate of group 0 to 1.9326e-01.\n",
      "e: 78 | Train Loss: 34426.2488 | 0.7684 | Acc: 0.8880\n",
      "e: 78 | Valid Loss: 6073.6666 | 1.5365 | Acc: 0.8174\n",
      "Epoch 00069: adjusting learning rate of group 0 to 1.9306e-01.\n",
      "e: 79 | Train Loss: 33908.2895 | 0.7568 | Acc: 0.8895\n",
      "e: 79 | Valid Loss: 6193.9208 | 1.5669 | Acc: 0.8168\n",
      "Epoch 00070: adjusting learning rate of group 0 to 1.9286e-01.\n",
      "e: 80 | Train Loss: 34787.4164 | 0.7764 | Acc: 0.8862\n",
      "e: 80 | Valid Loss: 5962.2905 | 1.5083 | Acc: 0.8148\n",
      "Validation loss decreased (6014.838707 --> 5962.290540).\n",
      "Epoch 00071: adjusting learning rate of group 0 to 1.9266e-01.\n",
      "e: 81 | Train Loss: 34487.7342 | 0.7697 | Acc: 0.8868\n",
      "e: 81 | Valid Loss: 6183.1995 | 1.5642 | Acc: 0.8168\n",
      "Epoch 00072: adjusting learning rate of group 0 to 1.9246e-01.\n",
      "e: 82 | Train Loss: 34905.5838 | 0.7791 | Acc: 0.8863\n",
      "e: 82 | Valid Loss: 6086.5599 | 1.5397 | Acc: 0.8186\n",
      "Epoch 00073: adjusting learning rate of group 0 to 1.9225e-01.\n",
      "e: 83 | Train Loss: 33783.6723 | 0.7540 | Acc: 0.8892\n",
      "e: 83 | Valid Loss: 6097.0890 | 1.5424 | Acc: 0.8166\n",
      "Epoch 00074: adjusting learning rate of group 0 to 1.9204e-01.\n",
      "e: 84 | Train Loss: 34875.0479 | 0.7784 | Acc: 0.8859\n",
      "e: 84 | Valid Loss: 6140.8928 | 1.5535 | Acc: 0.8174\n",
      "Epoch 00075: adjusting learning rate of group 0 to 1.9183e-01.\n",
      "e: 85 | Train Loss: 34451.9487 | 0.7689 | Acc: 0.8870\n",
      "e: 85 | Valid Loss: 6176.9741 | 1.5626 | Acc: 0.8163\n",
      "Epoch 00076: adjusting learning rate of group 0 to 1.9161e-01.\n",
      "e: 86 | Train Loss: 34422.9952 | 0.7683 | Acc: 0.8864\n",
      "e: 86 | Valid Loss: 6146.7529 | 1.5550 | Acc: 0.8166\n",
      "Epoch 00077: adjusting learning rate of group 0 to 1.9139e-01.\n",
      "e: 87 | Train Loss: 34304.3078 | 0.7656 | Acc: 0.8870\n",
      "e: 87 | Valid Loss: 6065.6375 | 1.5344 | Acc: 0.8148\n",
      "Epoch 00078: adjusting learning rate of group 0 to 1.9117e-01.\n",
      "e: 88 | Train Loss: 33717.6838 | 0.7525 | Acc: 0.8877\n",
      "e: 88 | Valid Loss: 6213.2145 | 1.5718 | Acc: 0.8171\n",
      "Epoch 00079: adjusting learning rate of group 0 to 1.9095e-01.\n",
      "e: 89 | Train Loss: 34203.6355 | 0.7634 | Acc: 0.8888\n",
      "e: 89 | Valid Loss: 6128.1584 | 1.5503 | Acc: 0.8179\n",
      "Epoch 00080: adjusting learning rate of group 0 to 1.9072e-01.\n",
      "e: 90 | Train Loss: 34282.6895 | 0.7652 | Acc: 0.8857\n",
      "e: 90 | Valid Loss: 6182.8249 | 1.5641 | Acc: 0.8174\n",
      "Epoch 00081: adjusting learning rate of group 0 to 1.9049e-01.\n",
      "e: 91 | Train Loss: 34982.9771 | 0.7808 | Acc: 0.8853\n",
      "e: 91 | Valid Loss: 6179.3360 | 1.5632 | Acc: 0.8179\n",
      "Epoch 00082: adjusting learning rate of group 0 to 1.9026e-01.\n",
      "e: 92 | Train Loss: 34145.5482 | 0.7621 | Acc: 0.8879\n",
      "e: 92 | Valid Loss: 6149.5119 | 1.5557 | Acc: 0.8156\n",
      "Epoch 00083: adjusting learning rate of group 0 to 1.9003e-01.\n",
      "e: 93 | Train Loss: 34228.8264 | 0.7640 | Acc: 0.8874\n",
      "e: 93 | Valid Loss: 6095.0700 | 1.5419 | Acc: 0.8179\n",
      "Epoch 00084: adjusting learning rate of group 0 to 1.8979e-01.\n",
      "e: 94 | Train Loss: 33122.0145 | 0.7392 | Acc: 0.8912\n",
      "e: 94 | Valid Loss: 6381.7559 | 1.6144 | Acc: 0.8174\n",
      "Epoch 00085: adjusting learning rate of group 0 to 1.8956e-01.\n",
      "e: 95 | Train Loss: 33599.5588 | 0.7499 | Acc: 0.8888\n",
      "e: 95 | Valid Loss: 6217.7986 | 1.5729 | Acc: 0.8163\n",
      "Epoch 00086: adjusting learning rate of group 0 to 1.8931e-01.\n",
      "e: 96 | Train Loss: 33943.7592 | 0.7576 | Acc: 0.8881\n",
      "e: 96 | Valid Loss: 6199.6491 | 1.5683 | Acc: 0.8186\n",
      "Epoch 00087: adjusting learning rate of group 0 to 1.8907e-01.\n",
      "e: 97 | Train Loss: 34057.1534 | 0.7601 | Acc: 0.8880\n",
      "e: 97 | Valid Loss: 6147.0330 | 1.5550 | Acc: 0.8176\n",
      "Epoch 00088: adjusting learning rate of group 0 to 1.8882e-01.\n",
      "e: 98 | Train Loss: 33848.7701 | 0.7555 | Acc: 0.8892\n",
      "e: 98 | Valid Loss: 6195.3809 | 1.5673 | Acc: 0.8171\n",
      "Epoch 00089: adjusting learning rate of group 0 to 1.8858e-01.\n",
      "e: 99 | Train Loss: 33254.8549 | 0.7422 | Acc: 0.8904\n",
      "e: 99 | Valid Loss: 6098.4481 | 1.5427 | Acc: 0.8163\n",
      "Epoch 00090: adjusting learning rate of group 0 to 1.8832e-01.\n",
      "e: 100 | Train Loss: 33830.8299 | 0.7551 | Acc: 0.8882\n",
      "e: 100 | Valid Loss: 6037.7386 | 1.5274 | Acc: 0.8171\n",
      "Epoch 00091: adjusting learning rate of group 0 to 1.8807e-01.\n",
      "e: 101 | Train Loss: 33633.6293 | 0.7507 | Acc: 0.8899\n",
      "e: 101 | Valid Loss: 6068.6062 | 1.5352 | Acc: 0.8181\n",
      "Test Loss: 6209.6358 | 1.5705 | Acc: 0.8101\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch_acc</td><td>▁▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇██████████████████</td></tr><tr><td>epoch_loss</td><td>█▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>running_loss</td><td>█▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>test_running_loss</td><td>▁</td></tr><tr><td>val_acc</td><td>▇▄▆▄▂▄▃▃▄▅▆▅▃▄▁▂▄▃▂▅▇▄▄▆▇▄▆▄▃▆▅▂▅▅▆▇█▅▆█</td></tr><tr><td>val_loss</td><td>██▇▇▇▇▆▅▆▄▅▄▅▃▅▄▃▄▄▄▄▄▃▃▄▄▄▃▃▅▄▁▃▄▄▄▃▄▄▂</td></tr><tr><td>val_running_loss</td><td>██▇▇▇▇▆▅▆▄▅▄▅▃▅▄▃▄▄▄▄▄▃▃▄▄▄▃▃▅▄▁▃▄▄▄▃▄▄▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>101</td></tr><tr><td>epoch_acc</td><td>0.88986</td></tr><tr><td>epoch_loss</td><td>0.75067</td></tr><tr><td>running_loss</td><td>33633.62929</td></tr><tr><td>test_acc</td><td>0.81007</td></tr><tr><td>test_loss</td><td>1.57047</td></tr><tr><td>test_running_loss</td><td>6209.63583</td></tr><tr><td>val_acc</td><td>0.81811</td></tr><tr><td>val_loss</td><td>1.53519</td></tr><tr><td>val_running_loss</td><td>6068.60619</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">ResNet50-SupCL-33-400-linear</strong>: <a href=\"https://wandb.ai/33h002/DeepFashion-pre-linear/runs/2f1oqot8\" target=\"_blank\">https://wandb.ai/33h002/DeepFashion-pre-linear/runs/2f1oqot8</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221201_184644-2f1oqot8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "from src.train import run_linearprotocol, run_test\n",
    "\n",
    "\n",
    "wandb.init(name=RUN_NAME, project=WANDB_PRJ, config=CONFIG, reinit=True)\n",
    "\n",
    "run_linearprotocol(model, mlp, dataloaders, dataset_sizes,\n",
    "                   optimizer, criterion, warmupscheduler, mainscheduler, device, RUN_NAME, SAVE_PATH)\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = timm.create_model('resnet50', pretrained=False, num_classes=num_classes)\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "\n",
    "mlp = MLP(num_classes=17)\n",
    "mlp.load_state_dict(torch.load(f'{SAVE_PATH}/{RUN_NAME}-MLP-best_model.pth'))  \n",
    "\n",
    "run_test(model, mlp, dataloaders, dataset_sizes, criterion, device)\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "soohee_env",
   "language": "python",
   "name": "soohee_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
